{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.15.0 tensorflow-gpu==1.15.0 stable_baselines gym box2d-py --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines import ACER\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmentName = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environmentName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-326.2848159128455\n",
      "Episode:2 Score:-140.37517452432877\n",
      "Episode:3 Score:-174.09486988073905\n",
      "Episode:4 Score:-138.0417722680746\n",
      "Episode:5 Score:-161.65087277886659\n",
      "Episode:6 Score:-192.75037573675235\n",
      "Episode:7 Score:-138.73493656187946\n",
      "Episode:8 Score:-132.0850043353526\n",
      "Episode:9 Score:-321.0606968009838\n",
      "Episode:10 Score:-272.51845500465777\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environmentName)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = ACER('MlpPolicy', env, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| avg_norm_adj        | 4.12e-11 |\n",
      "| avg_norm_g          | 17.8     |\n",
      "| avg_norm_grads_f    | 17.8     |\n",
      "| avg_norm_k          | 2.35e+05 |\n",
      "| avg_norm_k_dot_g    | 19.6     |\n",
      "| entropy             | 9.63     |\n",
      "| explained_variance  | -4.65    |\n",
      "| fps                 | 0        |\n",
      "| loss                | 32.7     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 3.3      |\n",
      "| loss_policy         | 3.3      |\n",
      "| loss_q              | 58.9     |\n",
      "| mean_episode_length | 0        |\n",
      "| mean_episode_reward | 0        |\n",
      "| norm_grads          | 106      |\n",
      "| norm_grads_policy   | 14.9     |\n",
      "| norm_grads_q        | 105      |\n",
      "| total_timesteps     | 20       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.529    |\n",
      "| avg_norm_g          | 6.02     |\n",
      "| avg_norm_grads_f    | 5.62     |\n",
      "| avg_norm_k          | 4.81     |\n",
      "| avg_norm_k_dot_g    | 5.74     |\n",
      "| entropy             | 9.74     |\n",
      "| explained_variance  | 0.165    |\n",
      "| fps                 | 503      |\n",
      "| loss                | 3.91     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -1.31    |\n",
      "| loss_policy         | -1.31    |\n",
      "| loss_q              | 10.6     |\n",
      "| mean_episode_length | 930      |\n",
      "| mean_episode_reward | 50.1     |\n",
      "| norm_grads          | 78.9     |\n",
      "| norm_grads_policy   | 19.2     |\n",
      "| norm_grads_q        | 76.5     |\n",
      "| total_timesteps     | 2020     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.acer.acer_simple.ACER at 0x22137aae630>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)#we need max(mean_episode_reward) and max(explained_variance)\n",
    "#when we get these values we can stop the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ACER_model\")#your_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ACER.load(\"ACER_model\", env=env)#your_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[161.35379]\n",
      "Episode:2 Score:[235.67111]\n",
      "Episode:3 Score:[96.40393]\n",
      "Episode:4 Score:[108.22351]\n",
      "Episode:5 Score:[213.20584]\n",
      "Episode:6 Score:[67.79252]\n",
      "Episode:7 Score:[227.16617]\n",
      "Episode:8 Score:[136.44415]\n",
      "Episode:9 Score:[138.24916]\n",
      "Episode:10 Score:[147.80466]\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    done = False\n",
    "    score = 0 \n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        env.render()\n",
    "        score+=rewards\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
